{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
    "# SPDX-License-Identifier: CC-BY-NC-SA-4.0\n",
    "#\n",
    "# This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike\n",
    "# 4.0 International License. https://creativecommons.org/licenses/by-nc-sa/4.0/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import paths  # noqa: F401\n",
    "import mediapy as media\n",
    "import torch\n",
    "from lightning.fabric import Fabric\n",
    "from l4p.utils.vis import generate_video_visualizations, generate_4D_visualization\n",
    "from l4p.models.utils import prepare_model\n",
    "from l4p.utils.viser import visualize_point_cloud_viser\n",
    "from l4p.data.davis import DavisDataset\n",
    "from l4p.data.video_dataset import VideoDataset\n",
    "from l4p.data.dycheck_dataset import DycheckDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision=\"16-mixed\"\n",
    "accelerator=\"gpu\"\n",
    "limit_gpu_mem_usage=False\n",
    "#limit_gpu_mem_usage=True  # set to True if Out of Memory\n",
    "vis_list = []\n",
    "vis_count = 0\n",
    "start_port = 8001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"l4p_depth_flow_2d3dtrack_camray_dynseg_v1\"\n",
    "ckpt_path = f\"../weights/{model_name}.ckpt\"\n",
    "model_config_path = \"../configs/model.yaml\"\n",
    "\n",
    "model = prepare_model(\n",
    "    model_config_path=model_config_path,\n",
    "    ckpt_path=ckpt_path,\n",
    "    max_queries=64 if limit_gpu_mem_usage else 128, # processes point tracking queries in batch of size max_queries\n",
    "    precision=precision,\n",
    "    accelerator=accelerator,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DAVIS Examples\n",
    "Tasks: depth, flow_2d_backward, dyn_mask, track_2d\n",
    "Davis provides instance masks, so we can sample and track points on the instance masks\n",
    "\"\"\"\n",
    "data_name = \"davis\"\n",
    "data_root = \"data/davis/v1\"\n",
    "test_dataset = DavisDataset(\n",
    "        data_root=data_root,\n",
    "        crop_size=(16,224,224) if limit_gpu_mem_usage else None,\n",
    "        estimation_directions=[1],\n",
    "        track_2d_querry_sampling_spacing=0.02,\n",
    "    )\n",
    "    \n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "fabric = Fabric(precision=precision, accelerator=accelerator)  # type: ignore\n",
    "test_dataloader = fabric.setup_dataloaders(test_dataloader)\n",
    "data_iter = iter(test_dataloader)\n",
    "\n",
    "# set tasks\n",
    "tasks = [\"depth\", \"flow_2d_backward\", \"dyn_mask\", \"track_2d\"]\n",
    "\n",
    "# Forward pass and visualize results\n",
    "for i in range(min(10,len(test_dataset))):\n",
    "    batch = next(data_iter)\n",
    "    print(\"Running inference for seq: \", batch[\"seq_name\"][0])\n",
    "    with torch.no_grad():\n",
    "        out = model.forward(batch, tasks)\n",
    "    if i == 0:\n",
    "        print(\"The model outputs: \", out.keys())\n",
    "    print(\"Generating visualization\")\n",
    "    out_path = os.path.join(\"results\",model_name,data_name)\n",
    "    out_vid, out_vid_name = generate_video_visualizations(batch, out, tasks, out_path)\n",
    "    media.show_video(out_vid, fps=15, border=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run on any general video\n",
    "Tasks: depth, flow_2d_backward, dyn_mask, track_2d\n",
    "\"\"\"\n",
    "# You can add your videos to data/videos\n",
    "data_name = \"videos\"\n",
    "video_paths = [\"data/davis/davis_train.mp4\", \"data/video/galileo.mp4\"]\n",
    "test_dataset = VideoDataset(\n",
    "        video_paths=video_paths,\n",
    "        crop_size=(16,224,224) if limit_gpu_mem_usage else (64,224,224),\n",
    "        estimation_directions=[1],\n",
    "        track_2d_querry_sampling_spacing=0.04,\n",
    "    )\n",
    "    \n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "fabric = Fabric(precision=precision, accelerator=accelerator)  # type: ignore\n",
    "test_dataloader = fabric.setup_dataloaders(test_dataloader)\n",
    "data_iter = iter(test_dataloader)\n",
    "\n",
    "# set tasks\n",
    "tasks = [\"depth\", \"flow_2d_backward\", \"dyn_mask\", \"track_2d\"]\n",
    "\n",
    "# Forward pass and visualize results\n",
    "for i in range(min(10,len(test_dataset))):\n",
    "    batch = next(data_iter)\n",
    "    print(\"Running inference for seq: \", batch[\"seq_name\"][0])\n",
    "    with torch.no_grad():\n",
    "        out = model.forward(batch, tasks)\n",
    "    if i == 0:\n",
    "        print(\"The model outputs: \", out.keys())\n",
    "    print(\"Generating visualization\")\n",
    "    out_path = os.path.join(\"results\",model_name,data_name)\n",
    "    out_vid, out_vid_name = generate_video_visualizations(batch, out, tasks, out_path)\n",
    "    media.show_video(out_vid, fps=15, border=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DAVIS Examples\n",
    "Tasks: depth, camray, flow_2d_backward, dyn_mask, track_2d\n",
    "DAVIS does not provide intrinsics, we estimate them as well.\n",
    "We can use the estimated cameras to visualize depth, cameras and 3D tracks in a consistent reference frame.\n",
    "\"\"\"\n",
    "data_name = \"davis\"\n",
    "data_root = \"data/davis/v2\"\n",
    "test_dataset = DavisDataset(\n",
    "        data_root=data_root,\n",
    "        crop_size=(16,224,224) if limit_gpu_mem_usage else (56,224,224),\n",
    "        estimation_directions=[1],\n",
    "        track_2d_querry_sampling_spacing=0.02,\n",
    "    )\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "fabric = Fabric(precision=precision, accelerator=accelerator)  # type: ignore\n",
    "test_dataloader = fabric.setup_dataloaders(test_dataloader)\n",
    "data_iter = iter(test_dataloader)\n",
    "\n",
    "# set tasks\n",
    "tasks = [\"depth\", \"flow_2d_backward\", \"dyn_mask\", \"track_2d\", \"camray\"]\n",
    "\n",
    "for i in range(min(10,len(test_dataset))):\n",
    "    batch = next(data_iter)\n",
    "    print(\"Running inference for seq: \", batch[\"seq_name\"][0])\n",
    "    with torch.no_grad():\n",
    "        out = model.forward(batch, tasks)\n",
    "    if i == 0:\n",
    "        print(\"The model outputs: \", out.keys())\n",
    "    print(\"Generating visualization\")\n",
    "    out_path = os.path.join(\"results\", \"4d_recon\", model_name, data_name)\n",
    "    out_vid, out_vid_name = generate_video_visualizations(batch, out, tasks, out_path)\n",
    "    media.show_video(out_vid, fps=15, border=True)\n",
    "    ply_files = generate_4D_visualization(batch, out, tasks, out_path)\n",
    "    if len(ply_files)>0:\n",
    "        port = start_port + vis_count # type: ignore\n",
    "        vis = visualize_point_cloud_viser(\n",
    "            ply_files, port = port, \n",
    "            seq_name = batch['seq_name'][0], # type: ignore\n",
    "            point_size = 0.02,)\n",
    "        vis_list.append(vis)\n",
    "        vis_count += 1 # type: ignore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Running on any general video\n",
    "Tasks: depth, camray, flow_2d_backward, dyn_mask, track_2d\n",
    "We can use the estimated cameras to visualize depth, cameras and 3D tracks in a consistent reference frame.\n",
    "\"\"\"\n",
    "data_name = \"videos\"\n",
    "video_paths = [\"data/davis/davis_train.mp4\", \"data/video/galileo.mp4\"]\n",
    "test_dataset = VideoDataset(\n",
    "        video_paths=video_paths,\n",
    "        crop_size=(16,224,224) if limit_gpu_mem_usage else (64,224,224),\n",
    "        estimation_directions=[1],\n",
    "        track_2d_querry_sampling_spacing=0.04,\n",
    "    )\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "fabric = Fabric(precision=precision, accelerator=accelerator)  # type: ignore\n",
    "test_dataloader = fabric.setup_dataloaders(test_dataloader)\n",
    "data_iter = iter(test_dataloader)\n",
    "\n",
    "# set tasks\n",
    "tasks = [\"depth\", \"flow_2d_backward\", \"dyn_mask\", \"track_2d\", \"camray\"]\n",
    "\n",
    "for i in range(min(10,len(test_dataset))):\n",
    "    batch = next(data_iter)\n",
    "    print(\"Running inference for seq: \", batch[\"seq_name\"][0])\n",
    "    with torch.no_grad():\n",
    "        out = model.forward(batch, tasks)\n",
    "    if i == 0:\n",
    "        print(\"The model outputs: \", out.keys())\n",
    "    print(\"Generating visualization\")\n",
    "    out_path = os.path.join(\"results\", \"4d_recon\", model_name, data_name)\n",
    "    out_vid, out_vid_name = generate_video_visualizations(batch, out, tasks, out_path)\n",
    "    media.show_video(out_vid, fps=15, border=True)\n",
    "    ply_files = generate_4D_visualization(batch, out, tasks, out_path)\n",
    "    if len(ply_files)>0:\n",
    "        port = start_port + vis_count # type: ignore\n",
    "        vis = visualize_point_cloud_viser(\n",
    "            ply_files, port = port, \n",
    "            seq_name = batch['seq_name'][0], # type: ignore\n",
    "            point_size = 0.02,)\n",
    "        vis_list.append(vis)\n",
    "        vis_count += 1 # type: ignore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Running on Dycheck dataset with input camera intrinsics\n",
    "Tasks: depth, camray, flow_2d_backward, dyn_mask, track_2d\n",
    "We can visualize the estimated cameras and depth in a consistent reference frame.\n",
    "\"\"\"\n",
    "original_flag_use_intrinsics = model.l4p_model.task_heads[\"camray\"].use_intrinsics\n",
    "model.l4p_model.task_heads[\"camray\"].use_intrinsics = True # uses input camera intrinsics\n",
    "\n",
    "data_name = \"dycheck\"\n",
    "data_root = \"data/dycheck/extracted\"\n",
    "test_dataset = DycheckDataset(\n",
    "    data_root=data_root,\n",
    "    resize_size=(298, 224),  # resize but keep aspect ratio\n",
    "    crop_size=(16,224,224) if limit_gpu_mem_usage else (64,224,224),\n",
    "    stride=2,\n",
    "    track_2d_querry_sampling_spacing=0.04,\n",
    ")\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "fabric = Fabric(precision=precision, accelerator=accelerator)  # type: ignore\n",
    "test_dataloader = fabric.setup_dataloaders(test_dataloader)\n",
    "data_iter = iter(test_dataloader)\n",
    "\n",
    "# set tasks\n",
    "tasks = [\"depth\", \"flow_2d_backward\", \"dyn_mask\", \"track_2d\", \"camray\"]\n",
    "\n",
    "for i in range(min(10,len(test_dataset))):\n",
    "    batch = next(data_iter)\n",
    "    print(\"Running inference for seq: \", batch[\"seq_name\"][0])\n",
    "    with torch.no_grad():\n",
    "        out = model.forward(batch, tasks)\n",
    "    if i == 0:\n",
    "        print(\"The model outputs: \", out.keys())\n",
    "    print(\"Generating visualization\")\n",
    "    out_path = os.path.join(\"results\", \"4d_recon\", model_name, data_name)\n",
    "    out_vid, out_vid_name = generate_video_visualizations(batch, out, tasks, out_path)\n",
    "    media.show_video(out_vid, fps=15, border=True)\n",
    "    ply_files = generate_4D_visualization(batch, out, tasks, out_path)\n",
    "    if len(ply_files)>0:\n",
    "        port = start_port + vis_count # type: ignore\n",
    "        vis = visualize_point_cloud_viser(\n",
    "            ply_files, port = port, \n",
    "            seq_name = batch['seq_name'][0], # type: ignore\n",
    "            point_size = 0.02,)\n",
    "        vis_list.append(vis)\n",
    "        vis_count += 1 # type: ignore\n",
    "\n",
    "model.l4p_model.task_heads[\"camray\"].use_intrinsics = original_flag_use_intrinsics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the viser server\n",
    "for vis in vis_list:\n",
    "    vis.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
